{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IU0FmRi7Sw7h"
   },
   "source": [
    "https://github.com/lucidrains/perceiver-ar-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L82QbwLTPclr"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/asigalov61/tegridy-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQwiG4rrUXba"
   },
   "outputs": [],
   "source": [
    "!pip install einops\n",
    "!pip install torch-summary\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L6S9kv7hlTr"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import secrets\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchsummary import summary\n",
    "from sklearn import metrics\n",
    "\n",
    "%cd /notebooks/tegridy-tools/tegridy-tools/\n",
    "\n",
    "import TMIDIX\n",
    "\n",
    "%cd /notebooks/tegridy-tools/tegridy-tools/Perceiver-AR/\n",
    "\n",
    "from perceiver_ar_pytorch import PerceiverAR\n",
    "from autoregressive_wrapper import AutoregressiveWrapper\n",
    "\n",
    "%cd /notebooks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_addr = \"/notebooks/Euterpe-INTs\"\n",
    "# os.chdir(dataset_addr)\n",
    "filez = list()\n",
    "for (dirpath, dirnames, filenames) in os.walk(dataset_addr):\n",
    "    filez += [os.path.join(dirpath, file) for file in filenames]\n",
    "print('=' * 70)\n",
    "\n",
    "filez.sort()\n",
    "\n",
    "print('Processing MIDI files. Please wait...')\n",
    "\n",
    "train_data = torch.Tensor()\n",
    "\n",
    "for f in tqdm.tqdm(filez):\n",
    "    train_data = torch.cat((train_data, torch.Tensor(pickle.load(open(f, 'rb')))))\n",
    "    print('Loaded file:', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:15], train_data[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model\n",
    "\n",
    "# constants\n",
    "\n",
    "SEQ_LEN = 8192 * 4 # 32k\n",
    "PREFIX_SEQ_LEN = (8192 * 4) - 1024\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "NUM_BATCHES = len(train_data) // SEQ_LEN // BATCH_SIZE\n",
    "\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "VALIDATE_EVERY  = 100\n",
    "GENERATE_EVERY  = 200\n",
    "SAVE_EVERY = 1000\n",
    "GENERATE_LENGTH = 32\n",
    "\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "# instantiate model\n",
    "\n",
    "model = PerceiverAR(\n",
    "    num_tokens = 512,\n",
    "    dim = 1024,\n",
    "    depth = 16,\n",
    "    heads = 8,\n",
    "    dim_head = 64,\n",
    "    cross_attn_dropout = 0.5,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    cross_attn_seq_len = PREFIX_SEQ_LEN\n",
    ")\n",
    "\n",
    "model = AutoregressiveWrapper(model)\n",
    "model.cuda()\n",
    "\n",
    "print('Done!')\n",
    "      \n",
    "summary(model)\n",
    "\n",
    "# prepare enwik8 data\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # random sampling\n",
    "        # idx = secrets.randbelow((self.data.size(0) // (self.seq_len))-1) * (self.seq_len)\n",
    "        \n",
    "        # consequtive sampling seems to be better at 64k seq_len\n",
    "        idx = index * self.seq_len\n",
    "        \n",
    "        full_seq = self.data[idx: idx + self.seq_len + 1].long()\n",
    "        return full_seq.cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.data.size(0) // self.seq_len)-1\n",
    "\n",
    "train_dataset = MusicDataset(train_data, SEQ_LEN)\n",
    "val_dataset   = MusicDataset(train_data, SEQ_LEN)\n",
    "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
    "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BoaqJAOU5M8"
   },
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QauuLTxFTDXw"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='Training'):\n",
    "    model.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        loss, acc = model(next(train_loader))\n",
    "        loss.backward()\n",
    "\n",
    "    print(f'Training loss: {loss.item()}')\n",
    "    print(f'Training acc: {acc.item()}')\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    train_accs.append(acc.item())\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc = model(next(val_loader))\n",
    "            print(f'Validation loss: {val_loss.item()}')\n",
    "            print(f'Validation acc: {val_acc.item()}')\n",
    "            val_losses.append(val_loss.item())\n",
    "            val_accs.append(val_acc.item())\n",
    "            \n",
    "            print('Plotting training loss graph...')\n",
    "            \n",
    "            tr_loss_list = train_losses\n",
    "            plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "            plt.show()\n",
    "            # plt.savefig('/notebooks/training_loss_graph.png')\n",
    "            plt.close()\n",
    "            print('Done!')\n",
    "            \n",
    "            print('Plotting training acc graph...')\n",
    "            \n",
    "            tr_loss_list = train_accs\n",
    "            plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "            plt.show()\n",
    "            # plt.savefig('/notebooks/training_acc_graph.png')\n",
    "            plt.close()\n",
    "            print('Done!')\n",
    "            \n",
    "            print('Plotting validation loss graph...')\n",
    "            tr_loss_list = val_losses\n",
    "            plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "            plt.show()\n",
    "            # plt.savefig('/notebooks/validation_loss_graph.png')\n",
    "            plt.close()\n",
    "            print('Done!')\n",
    "            \n",
    "            print('Plotting validation acc graph...')\n",
    "            tr_loss_list = val_accs\n",
    "            plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "            plt.show()\n",
    "            # plt.savefig('/notebooks/validation_accs_graph.png')\n",
    "            plt.close()\n",
    "            print('Done!')\n",
    "\n",
    "    if i % GENERATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        inp = random.choice(val_dataset)[:-1]\n",
    "        \n",
    "        print(inp)\n",
    "\n",
    "        sample = model.generate(inp[None, ...], GENERATE_LENGTH)\n",
    "        \n",
    "        print(sample)\n",
    "        \n",
    "    if i % SAVE_EVERY == 0:\n",
    "        \n",
    "        print('Saving model progress. Please wait...')\n",
    "        print('model_checkpoint_' + str(i) + '_steps_' + str(round(float(train_losses[-1]), 4)) + '_loss.pth')\n",
    "        \n",
    "        fname = '/notebooks/model_checkpoint_'  + str(i) + '_steps_' + str(round(float(train_losses[-1]), 4)) + '_loss.pth'\n",
    "        \n",
    "        torch.save(model.state_dict(), fname)\n",
    "        \n",
    "        # torch.save({'state_dict': model.state_dict(),\n",
    "        #             'optimizer': optim.state_dict(),\n",
    "        #            }, fname)\n",
    "        \n",
    "        print('Done!')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training loss graph\n",
    "\n",
    "plt.plot([i for i in range(len(train_losses))] ,train_losses, 'b')\n",
    "plt.savefig('/notebooks/training_loss_graph.png')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation loss graph\n",
    "\n",
    "plt.plot([i for i in range(len(val_losses))] ,val_losses, 'b')\n",
    "plt.savefig('/notebooks/validation_loss_graph.png')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load/Reload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "# constants\n",
    "\n",
    "SEQ_LEN = 8192 * 4 # 32k\n",
    "PREFIX_SEQ_LEN = (8192 * 4) - 1024\n",
    "\n",
    "model = PerceiverAR(\n",
    "    num_tokens = 512,\n",
    "    dim = 1024,\n",
    "    depth = 16,\n",
    "    heads = 8,\n",
    "    dim_head = 64,\n",
    "    cross_attn_dropout = 0.5,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    cross_attn_seq_len = PREFIX_SEQ_LEN\n",
    ")\n",
    "model = AutoregressiveWrapper(model)\n",
    "model.cuda()\n",
    "\n",
    "state_dict = torch.load('model_checkpoint_2000_steps_1.218_loss.pth')\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Model stats\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Token Embeddings\n",
    "\n",
    "cos_sim = metrics.pairwise.cosine_similarity(\n",
    "   model.net.token_emb.weight.detach().cpu().numpy()\n",
    ")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cos_sim, cmap=\"inferno\", interpolation=\"none\")\n",
    "im_ratio = cos_sim.shape[0] / cos_sim.shape[1]\n",
    "plt.colorbar(fraction=0.046 * im_ratio, pad=0.04)\n",
    "plt.xlabel(\"Position\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.tight_layout()\n",
    "plt.plot()\n",
    "plt.savefig(\"/notebooks/Euterpe-Positional-Embeddings-Plot.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-Nh0Vw6hqhi"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model.eval()\n",
    "inp = val_dataset[0][:-512]\n",
    "\n",
    "print(inp)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "out = model.generate(inp[None, ...], \n",
    "                     512, \n",
    "                     temperature=1)\n",
    "\n",
    "print(time.time() - start_time, \"seconds\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evZLAi-8e_Ok"
   },
   "outputs": [],
   "source": [
    "out1 = out.cpu().tolist()[0]\n",
    "\n",
    "if len(out1) != 0:\n",
    "    \n",
    "    song = out1\n",
    "    song_f = []\n",
    "    time = 0\n",
    "    dur = 0\n",
    "    vel = 0\n",
    "    pitch = 0\n",
    "    channel = 0\n",
    "\n",
    "    son = []\n",
    "\n",
    "    song1 = []\n",
    "\n",
    "    for s in song:\n",
    "      if s > 127:\n",
    "        son.append(s)\n",
    "\n",
    "      else:\n",
    "        if len(son) == 4:\n",
    "          song1.append(son)\n",
    "        son = []\n",
    "        son.append(s)\n",
    "    \n",
    "    for s in song1:\n",
    "\n",
    "        channel = s[0] // 11\n",
    "\n",
    "        vel = (s[0] % 11) * 19\n",
    "\n",
    "        time += (s[1]-128) * 16\n",
    "            \n",
    "        dur = (s[2] - 256) * 32\n",
    "        \n",
    "        pitch = (s[3] - 384)\n",
    "                                  \n",
    "        song_f.append(['note', time, dur, channel, pitch, vel ])\n",
    "\n",
    "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
    "                                                        output_signature = 'Euterpe',  \n",
    "                                                        output_file_name = '/notebooks/Euterpe-Music-Composition', \n",
    "                                                        track_name='Project Los Angeles',\n",
    "                                                        list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 16, 0, 0, 0, 0],\n",
    "                                                        number_of_ticks_per_quarter=500)\n",
    "\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
