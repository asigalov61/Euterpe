{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IU0FmRi7Sw7h"
   },
   "source": [
    "https://github.com/lucidrains/sinkhorn-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L82QbwLTPclr"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAVW-nGThSmG"
   },
   "outputs": [],
   "source": [
    "!pip install sinkhorn_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQwiG4rrUXba"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/asigalov61/tegridy-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L6S9kv7hlTr"
   },
   "outputs": [],
   "source": [
    "%cd /notebooks/tegridy-tools/tegridy-tools/\n",
    "import TMIDIX\n",
    "%cd /notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import tqdm\n",
    "import torch \n",
    "\n",
    "dataset_addr = \"/notebooks/Euterpe-INTs\"\n",
    "# os.chdir(dataset_addr)\n",
    "filez = list()\n",
    "for (dirpath, dirnames, filenames) in os.walk(dataset_addr):\n",
    "    filez += [os.path.join(dirpath, file) for file in filenames]\n",
    "print('=' * 70)\n",
    "\n",
    "filez.sort()\n",
    "\n",
    "print('Processing MIDI files. Please wait...')\n",
    "\n",
    "train_data = torch.Tensor()\n",
    "\n",
    "for f in tqdm.tqdm(filez):\n",
    "    train_data = torch.cat((train_data, torch.Tensor(pickle.load(open(f, 'rb')))))\n",
    "    print('Loaded file:', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:15], train_data[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BoaqJAOU5M8"
   },
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geiuHa00jj8Y"
   },
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QauuLTxFTDXw"
   },
   "outputs": [],
   "source": [
    "from sinkhorn_transformer import SinkhornTransformerLM\n",
    "from sinkhorn_transformer.autoregressive_wrapper import AutoregressiveWrapper\n",
    "\n",
    "import random\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import secrets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# constants\n",
    "\n",
    "SEQ_LEN = 4096 # 4096\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "NUM_BATCHES = len(train_data) // SEQ_LEN // BATCH_SIZE\n",
    "\n",
    "GRADIENT_ACCUMULATE_EVERY = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATE_EVERY  = 50\n",
    "GENERATE_EVERY  = 150\n",
    "SAVE_EVERY = 50\n",
    "GENERATE_LENGTH = 32\n",
    "\n",
    "\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "# instantiate model\n",
    "\n",
    "model = SinkhornTransformerLM(\n",
    "    num_tokens = 512,\n",
    "    emb_dim = 128,\n",
    "    dim = 1024,\n",
    "    depth = 24,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    heads = 8,\n",
    "    bucket_size = 128,\n",
    "    ff_chunks = 4,\n",
    "    causal = True,\n",
    "    reversible = True,\n",
    "    attn_dropout = 0.1,\n",
    "    n_local_attn_heads = 4,\n",
    ")\n",
    "\n",
    "model = AutoregressiveWrapper(model)\n",
    "model.cuda()\n",
    "\n",
    "# prepare enwik8 data\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        idx = secrets.randbelow((self.data.size(0) // (self.seq_len))-1) * (self.seq_len)\n",
    "        \n",
    "        full_seq = self.data[idx: idx + self.seq_len + 1].long()\n",
    "        return full_seq.cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.data.size(0) // self.seq_len)-1\n",
    "\n",
    "train_dataset = MusicDataset(train_data, SEQ_LEN)\n",
    "val_dataset   = MusicDataset(train_data, SEQ_LEN)\n",
    "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
    "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# training\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
    "    model.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        loss = model(next(train_loader), return_loss = True)\n",
    "        loss.backward()\n",
    "\n",
    "    print(f'training loss: {loss.item()}')\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = model(next(val_loader), return_loss = True)\n",
    "            print(f'validation loss: {val_loss.item()}')\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            print('Saving validation loss graph...')\n",
    "            tr_loss_list = val_losses\n",
    "            plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "            plt.show()\n",
    "            # plt.savefig('/notebooks/validation_loss_graph.png')\n",
    "            plt.close()\n",
    "            print('Done!')\n",
    "\n",
    "    if i % GENERATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        inp = random.choice(val_dataset)[:-1]\n",
    "        \n",
    "        print(f'%s \\n\\n %s', (inp, '*' * 100))\n",
    "\n",
    "        sample = model.generate(inp, GENERATE_LENGTH)\n",
    "        \n",
    "        print(sample)\n",
    "        \n",
    "    if i % SAVE_EVERY == 0:\n",
    "        \n",
    "        print('Saving model progress. Please wait...')\n",
    "        print('model_checkpoint_' + str(i) + '_steps_' + str(round(float(train_losses[-1]), 4)) + '_loss.pth')\n",
    "        torch.save(model.state_dict(), '/notebooks/model_checkpoint_'  + str(i) + '_steps_' + str(round(float(train_losses[-1]), 4)) + '_loss.pth')\n",
    "        print('Done!')\n",
    "        \n",
    "        print('Saving training loss graph...')\n",
    "        tr_loss_list = train_losses\n",
    "        plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "        plt.show()\n",
    "        # plt.savefig('/notebooks/training_loss_graph.png')\n",
    "        plt.close()\n",
    "        print('Done!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(train_losses))] ,train_losses, 'b')\n",
    "plt.savefig('/notebooks/training_loss_graph.png')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(val_losses))] ,val_losses, 'b')\n",
    "plt.savefig('/notebooks/validation_loss_graph.png')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-Nh0Vw6hqhi"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "inp = val_dataset[2][:512]\n",
    "\n",
    "print(f'%s \\n\\n %s', (inp, '*' * 100))\n",
    "# torch.LongTensor([6]).cuda()\n",
    "start_time = time.time()\n",
    "out = model.generate(inp, 512)\n",
    "print(time.time() - start_time, \"seconds\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuwPkmFXhrI2"
   },
   "outputs": [],
   "source": [
    "out1 = out.cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evZLAi-8e_Ok"
   },
   "outputs": [],
   "source": [
    "if len(out1) != 0:\n",
    "    \n",
    "    song = out1\n",
    "    song_f = []\n",
    "    time = 0\n",
    "    dur = 0\n",
    "    vel = 0\n",
    "    pitch = 0\n",
    "    channel = 0\n",
    "\n",
    "    son = []\n",
    "\n",
    "    song1 = []\n",
    "\n",
    "    for s in song:\n",
    "      if s > 127:\n",
    "        son.append(s)\n",
    "\n",
    "      else:\n",
    "        if len(son) == 4:\n",
    "          song1.append(son)\n",
    "        son = []\n",
    "        son.append(s)\n",
    "    \n",
    "    for s in song1:\n",
    "\n",
    "        channel = s[0] // 11\n",
    "\n",
    "        vel = (s[0] % 11) * 19\n",
    "\n",
    "        time += (s[1]-128) * 16\n",
    "            \n",
    "        dur = (s[2] - 256) * 32\n",
    "        \n",
    "        pitch = (s[3] - 384)\n",
    "                                  \n",
    "        song_f.append(['note', time, dur, channel, pitch, vel ])\n",
    "\n",
    "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
    "                                                        output_signature = 'Mini Muse',  \n",
    "                                                        output_file_name = '/notebooks/Mini-Muse-Music-Composition', \n",
    "                                                        track_name='Project Los Angeles',\n",
    "                                                        list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 19, 0, 0, 0, 0],\n",
    "                                                        number_of_ticks_per_quarter=500)\n",
    "\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = train_data[:160000]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
